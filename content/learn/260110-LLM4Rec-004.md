---
title: "LLM4Rec-Learning-004: 生成式推荐体系结构"
date: 2026-01-10
draft: false
tags: ["大模型","推荐系统"]
categories: ["学习", "LLM4Rec"]
---

{{< quote >}}
生成式推荐大致的体系结构
{{< /quote >}}
<!--more-->

## 一、传统VS生成式

**传统范式**：`召回 -> 排序 -> 重排`。模型在给定候选池中计算概率（如CTR），是“**选择题**”。

**生成式范式**：将用户、物品、上下文全部“语言化”，模型基于历史和上下文，自回归地预测下一个物品ID序列，是“**填空题/作文题**”。推荐列表是一个“序列生成”任务，物品ID是“词汇”，用户行为史是“上下文”。

> 传统就是一种判别式的策略，而生成式策略主要是LLM的特性。

## 二、生成式推荐

### 核心技术点

1. **物品语义化与离散表示**：如何把一件商品或一个视频变成模型能“生成”的Token。
    - 重点学习：**语义ID（Semantic ID）**。这并非简单哈希，而是通过向量量化（VQ-VAE等）学到的、携带语义的离散编码序列。例如，一件“蓝色修身羽绒服”的ID可能是`[231, 586, 74]`，其中子编码分别对应“羽绒服”、“修身”、“蓝色”等语义。
    - 为什么重要？这是连接物品物理世界与生成模型语言世界的桥梁，决定了模型能学到多好的物品表征。


## 三、模型架构

### 3.1 整体上

- LLM As Rec
- LLM For Rec

### 3.2 大致架构

- **向量空间：**LLM作为embedding或者ReRank

- **生成式重排架构**：在传统召回、粗排后，用**生成式模型（通常是LLM）进行最终的重排和列表生成**。这是当前最易落地的方式，典型如**LLaRA**架构。学习重点：如何将用户历史、候选物品信息**格式化**为LLM能理解的提示（Prompt）。

- **纯生成式架构**：将整个推荐流水线统一为一个**自回归生成模型**（如GPT风格）。输入超长用户行为序列的语义ID，直接输出未来可能交互的物品ID序列。这是最彻底的范式转变，典型如快手的**OneRec**。
- **生成式智能体架构**：将LLM作为具有“记忆”、“规划”、“工具使用”能力的智能体。它不仅可以生成推荐，还可以**自我规划**（例如：“用户刚看了手机，下一步应该推荐手机壳，然后看看充电宝”），并调用传统搜索、评分预测等工具。这是最前沿、交互性最强的方向。


## 四、pipeline

- 整体流程

- 训练方式

    **训练数据构建**：需要构建类似于语言模型的“`(上下文, 下一个Token)`”样本对，但这里的Token是物品语义ID。关键在于如何构建高质量的“推荐语料”——即`(用户行为序列, 下一个物品)` 的配对。

    **训练目标**：

    - **自回归生成损失**：基本目标，即预测序列中下一个物品ID。
    - **多任务/指令精调**：为了让模型遵循推荐指令（如“推荐一些夏日清凉的连衣裙”），需要使用指令数据进行精调。
    - **强化学习与偏好对齐**：引入奖励模型，对齐人类偏好（如时长、点赞、多样性）。这与训练ChatGPT的RLHF过程神似，但奖励信号来自推荐场景。

- TIGER案例

- 数据集

- 评估

- SOTO


## 五、详细解读与实践
> 都是一篇篇论文啊

## 六、自学

- **论文**：紧盯 **KDD、RecSys、WWW** 等会议，搜索关键词 “**Generative Recommendation**”, “**LLM for Recommendation**”, “**Semantic ID**”。
- **实践**：从 **LLaRA** 这类混合架构开始尝试，用开源LLM（如LLaMA系列）在公开数据集（如MovieLens）上练习**Prompt构建和重排任务**，直观感受生成过程。
- **代码**：关注 **RecBole**（一个推荐系统库）是否已集成生成式模型模块，或GitHub上相关的开源实现
- **视频：**前沿的一些公开演讲、会议、组会等