import{_ as i,c as a,o as n,a2 as t}from"./chunks/framework.DA-Pb-tg.js";const o=JSON.parse('{"title":"0 torch.optim","description":"","frontmatter":{},"headers":[],"relativePath":"AI/01_deep_learning_theory/24-pytorch-optimizer.md","filePath":"AI/01_deep_learning_theory/24-pytorch-optimizer.md","lastUpdated":1743069065000}'),p={name:"AI/01_deep_learning_theory/24-pytorch-optimizer.md"};function l(h,s,k,e,r,d){return n(),a("div",null,s[0]||(s[0]=[t(`<h1 id="_0-torch-optim" tabindex="-1">0 torch.optim <a class="header-anchor" href="#_0-torch-optim" aria-label="Permalink to &quot;0 torch.optim&quot;">​</a></h1><p>         torch.optim 是一个实现各种优化算法的包。最常用的方法已经得到支持，并且接口足够通用，以便未来可以轻松地集成更复杂的方法。<br></p><ul><li><a href="https://pytorch.org/docs/stable/optim.html" target="_blank" rel="noreferrer">torch.optim guide link</a></li></ul><h1 id="_1-如何使用torch-optim" tabindex="-1">1 如何使用torch.optim <a class="header-anchor" href="#_1-如何使用torch-optim" aria-label="Permalink to &quot;1 如何使用torch.optim&quot;">​</a></h1><h2 id="_1-1-创建一个优化器对象" tabindex="-1">1.1 创建一个优化器对象 <a class="header-anchor" href="#_1-1-创建一个优化器对象" aria-label="Permalink to &quot;1.1 创建一个优化器对象&quot;">​</a></h2><p>        要使用 torch.optim，您需要构建一个优化器对象，该对象将<strong>保存当前状态</strong>并根据计算得到的梯度更新参数。<br></p><p><strong>思考：当前状态 指的是什么 ？？？</strong> <br></p><p>        要构建一个优化器（Optimizer），您需要提供一个可迭代的对象，其中包含要优化的参数（所有参数应该是 Parameter 类型）。然后，您可以指定优化器特定的选项，例如学习率（learning rate）、权重衰减（weight decay）等。<br></p><ul><li>代码展示 <br></li></ul><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">optimizer </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> optim.SGD(model.parameters(), </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">lr</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.01</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">momentum</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.9</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">optimizer </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> optim.Adam([var1, var2], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">lr</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.0001</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><h2 id="_1-2-逐参数选项-per-parameter-options" tabindex="-1">1.2 逐参数选项(Per-parameter options) <a class="header-anchor" href="#_1-2-逐参数选项-per-parameter-options" aria-label="Permalink to &quot;1.2 逐参数选项(Per-parameter options)&quot;">​</a></h2><p>        优化器还支持指定每个参数的选项。为了实现这一点，不要传递一个 Parameter 对象的可迭代对象，而是传递一个 dict 对象的可迭代对象。每个 dict 对象将定义一个单独的参数组，并且应包含一个 <strong>&quot;params&quot; key</strong>，其中包含属于该组的参数列表。<strong>其他 key</strong>应与优化器接受的关键字参数匹配，并将用作该组的优化选项。<br></p><ul><li>注意(Note) <br>         您仍然可以将选项作为关键字参数传递。它们将被用作默认值，在未覆盖它们的组中生效。当您只想改变单个选项，同时保持其他参数组之间一致时，这非常有用。<br></li></ul><p>        例如，当需要指定每个层的学习率时，这非常有用：<br></p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">optim.SGD([</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">                {</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;params&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: model.base.parameters()},</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">                {</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;params&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: model.classifier.parameters(), </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;lr&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1e-3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            ], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">lr</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1e-2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">momentum</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.9</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>        这意味着 model.base 的参数将使用默认学习率 1e-2，model.classifier 的参数将使用学习率 1e-3，并且所有参数都将使用动量 0.9。<br></p><h2 id="_1-3-进行优化步骤" tabindex="-1">1.3 进行优化步骤 <a class="header-anchor" href="#_1-3-进行优化步骤" aria-label="Permalink to &quot;1.3 进行优化步骤&quot;">​</a></h2><p>        所有的优化器都实现了 step() 方法，用于更新参数。它可以通过两种方式使用：<br></p><ul><li>方式1：<br><strong>optimizer.step()</strong> <br></li></ul><p>        这是大多数优化器支持的简化版本。可以在计算梯度（例如通过 backward()）之后调用该函数，具体代码为：<br></p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> input</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, target </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> dataset:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    optimizer.zero_grad()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    output </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    loss </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> loss_fn(output, target)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    loss.backward()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    optimizer.step()</span></span></code></pre></div><ul><li>方式2：<br><strong>optimizer.step(closure)</strong> <br></li></ul><p>        一些优化算法，如共轭梯度（Conjugate Gradient）和LBFGS，需要多次重新评估函数，因此您需要传递一个闭包（closure），使它们能够重新计算您的模型。闭包应该清除梯度、计算损失并返回它。<br></p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> input</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, target </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> dataset:</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> closure</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">():</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        optimizer.zero_grad()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        output </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        loss </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> loss_fn(output, target)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        loss.backward()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> loss</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    optimizer.step(closure)</span></span></code></pre></div><h1 id="_2-torch-optim-base-class-introduce" tabindex="-1">2 torch.optim base class introduce <a class="header-anchor" href="#_2-torch-optim-base-class-introduce" aria-label="Permalink to &quot;2 torch.optim base class introduce&quot;">​</a></h1><ul><li><a href="https://pytorch.org/docs/stable/_modules/torch/optim/optimizer.html#Optimizer" target="_blank" rel="noreferrer">CLASStorch.optim.Optimizer(params, defaults)</a> <br></li></ul><h2 id="_2-1-torch-optim-optimizer-的输入参数" tabindex="-1">2.1 torch.optim.Optimizer 的输入参数 <a class="header-anchor" href="#_2-1-torch-optim-optimizer-的输入参数" aria-label="Permalink to &quot;2.1 torch.optim.Optimizer 的输入参数&quot;">​</a></h2><ul><li>params(可迭代对象) <br>         一个包含 torch.Tensor 或 dict 的可迭代对象。指定应该进行优化的张量。</li></ul><p>        <strong>注意</strong> <br>         参数需要以具有确定性顺序且在运行之间保持一致的集合形式进行指定。不满足这些属性的对象的示例包括集合（sets）和对字典值进行迭代的迭代器（iterators）。<br></p><ul><li>defaults(字典) <br>         一个包含优化选项默认值的字典（在参数组未指定时使用）。</li></ul><h2 id="_2-2-torch-optim-optimizer-属性" tabindex="-1">2.2 torch.optim.Optimizer 属性 <a class="header-anchor" href="#_2-2-torch-optim-optimizer-属性" aria-label="Permalink to &quot;2.2 torch.optim.Optimizer 属性&quot;">​</a></h2><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.defaults </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> defaults </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 存储优化器的全局超参数</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">._optimizer_step_pre_hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OrderedDict() </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 存储 用于 step()方法 中的前置钩子函数</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">._optimizer_step_post_hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OrderedDict() </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 存储 用于step()方法 中的后置钩子函数</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">._optimizer_state_dict_pre_hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OrderedDict() </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 存储 用于 state_dict()方法中的 前置钩子函数</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">._optimizer_state_dict_post_hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OrderedDict() </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 存储 用于 state_dict()方法 中的 后置钩子函数</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">._optimizer_load_state_dict_pre_hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OrderedDict() </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 存储 用于load_state_dict() 方法中的前置钩子函数</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">._optimizer_load_state_dict_post_hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OrderedDict() </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 存储 用于load_state_dict() 方法中的后置钩子函数</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.state: DefaultDict[torch.Tensor, Any] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> defaultdict(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">dict</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 存储 优化器的状态</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.param_groups: List[Dict[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">str</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Any]] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [] </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 存储不同group 的 Parameter参数 和 其它超参数</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">param_groups </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> list</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(params) </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 输入参数转为 list</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> len</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param_groups) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># params 不能为空</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    raise</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> ValueError</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;optimizer got an empty parameter list&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">if</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> isinstance</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param_groups[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">dict</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 不是字典类型的话，说明是 params 组成的可迭代对象</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    param_groups </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [{</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;params&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: param_groups}]  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 将 params的可迭代对象转换为 字典形式</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> param_group </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> param_groups:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.add_param_group(cast(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">dict</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, param_group)) </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 将param_groups 中每个group 添加到 self.param_groups 中</span></span></code></pre></div><p><strong>思考：上述属性那几个属性最为关键 ？？？</strong> <br></p><h2 id="_2-3-torch-optim-optimizer-方法" tabindex="-1">2.3 torch.optim.Optimizer 方法 <a class="header-anchor" href="#_2-3-torch-optim-optimizer-方法" aria-label="Permalink to &quot;2.3 torch.optim.Optimizer 方法&quot;">​</a></h2><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 构造函数：主要完成self.defaults self.state self.param_groups 的初始化</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> __init__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, params: params_t, defaults: Dict[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">str</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Any]) -&gt; </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 获取 optimize 的状态，序列化时会触发</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> __getstate__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self) -&gt; Dict[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">str</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Any]:</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 反序列化时触发，调用 pickle.load(file) 或 pickle.loads(bytes) 时，加载保存的模型状态</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> __setstate__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, state: Dict[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">str</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Any]) -&gt; </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 当我们在交互式环境中输出对象或使用 repr() 函数时，会自动调用对象的 __repr__ 方法来获取其字符串表示。</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> __repr__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self) -&gt; </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">str</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 用于pytorch2.0 torch.compile 时的加速，避免一些耗时的检查</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> _cuda_graph_capture_health_check</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self) -&gt; </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># torch.profile.profiler的入口点，被 profile_hook_step 方法调用</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> _optimizer_step_code</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self) -&gt; </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 静态方法：返回一个装饰器 wrapper，用于插入 钩子函数 和 _optimizer_step_code 函数， 被 _patch_step_function 调用</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">staticmethod</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> profile_hook_step</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(func: Callable[_P, R]) -&gt; Callable[_P, R]:</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 按设备和数据类型对张量的列表进行分组。</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 如果我们正在编译，则跳过此步骤，因为这将在inductor lowering 阶段发生。</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">staticmethod</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> _group_tensors_by_device_and_dtype</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    tensorlistlist: TensorListList,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    with_indices: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">bool</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) -&gt; Union[</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    Dict[Tuple[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], Tuple[TensorListList, Indices]],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    Dict[Tuple[torch.device, torch.dtype], Tuple[TensorListList, Indices]],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 通过wrapper的方式给step方法加补丁(patch)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> _patch_step_function</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self) -&gt; </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 注册一个优化器 step 前 的前置钩子函数，该钩子将在优化器步骤之前调用。它应该具有以下签名：</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># hook(optimizer, args, kwargs) -&gt; None or modified args and kwargs</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> register_step_pre_hook</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, hook: OptimizerPreHook) -&gt; RemovableHandle:</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 注册一个优化器 step 操作后的 后置钩子函数，该钩子将在优化器步骤之后调用。它应该具有以下签名：</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># hook(optimizer, args, kwargs) -&gt; None or modified args and kwargs</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> register_step_post_hook</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, hook: OptimizerPostHook) -&gt; RemovableHandle:</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 注册一个状态字典前置钩子，该钩子将在调用torch.optim.Optimizer.state_dict之前被调用。它应该具有以下签名：</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># hook(optimizer) -&gt; None</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># optimizer参数是正在使用的优化器实例。在调用self上的state_dict之前，将使用参数self调用该钩子。注册的钩子可用于在进行state_dict调用之前执行预处理操作。</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> register_state_dict_pre_hook</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    self, hook: Callable[[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Optimizer&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], prepend: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">bool</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> False</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) -&gt; RemovableHandle:</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 注册一个状态字典后置钩子，该钩子将在调用:torch.optim.Optimizer.state_dict之后被调用。它应该具有以下签名：</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># hook(optimizer, state_dict) -&gt; state_dict or None</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 在self上生成state_dict之后，将使用参数self和state_dict调用该钩子。</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 该钩子可以就地修改state_dict，或者可选择返回一个新的state_dict。注册的钩子可用于在返回state_dict之前对其进行后处理。</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> register_state_dict_post_hook</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    self,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    hook: Callable[[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Optimizer&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, StateDict], Optional[StateDict]],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    prepend: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">bool</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) -&gt; RemovableHandle:</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 以class:dict的形式返回优化器的状态。</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 它包含两个条目：</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 1. state：保存当前优化状态的字典。不同的优化器类之间其内容可能不同，但有一些共同的特征。</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 例如，状态是按参数保存的，而参数本身不会被保存。state是一个将参数ID映射到相应参数状态字典的字典。</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 2. param_groups：包含所有参数组的列表，其中每个参数组是一个字典。</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 每个参数组包含特定于优化器的元数据，如学习率和权重衰减，以及参数组中参数的参数ID列表。</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 参数ID可能看起来像索引，但它们只是将状态与参数组关联起来的ID</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@torch._disable_dynamo</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> state_dict</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self) -&gt; StateDict:</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 根据param 规则进行 Tensor value 的转换</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">staticmethod</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> _process_value_according_to_param_policy</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    param: torch.Tensor,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    value: torch.Tensor,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    param_id: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    param_groups: List[Dict[Any, Any]],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    key: Hashable </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) -&gt; torch.Tensor:</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 注册一个 load_state_dict 前置钩子，在调用torch.optim.Optimizer.load_state_dict 之前将会被调用。它应该具有以下的签名：</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># hook(optimizer, state_dict) -&gt; state_dict or None</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># optimizer 参数是正在使用的优化器实例，state_dict 参数是用户传递给 load_state_dict 的 state_dict 的浅拷贝。</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 预钩子可以就地修改 state_dict，或者可选择返回一个新的 state_dict。如果返回了一个 state_dict，它将被用于加载到优化器中。</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 在调用 self 上的 load_state_dict 之前，钩子将以 self 和 state_dict 作为参数被调用。</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 注册的钩子可以用于在进行 load_state_dict 调用之前执行预处理操作。</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> register_load_state_dict_pre_hook</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    self,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    hook: Callable[[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Optimizer&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, StateDict], Optional[StateDict]],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    prepend: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">bool</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) -&gt; RemovableHandle:</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 注册一个 load_state_dict 后钩子，在调用torch.optim.Optimizer.load_state_dict 之后将会被调用。它应该具有以下的签名：</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># hook(optimizer) -&gt; None</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># optimizer 参数是正在使用的优化器实例。</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 在调用 self 上的 load_state_dict 之后，钩子将以 self 作为参数被调用。注册的钩子可以用于在 load_state_dict 加载完 state_dict 后执行后处理操作。</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> register_load_state_dict_post_hook</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    self, hook: Callable[[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Optimizer&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], prepend: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">bool</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> False</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) -&gt; RemovableHandle:</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 加载存储的优化器状态</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@torch._disable_dynamo</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> load_state_dict</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, state_dict: StateDict) -&gt; </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 重置所有被优化的 torch.Tensor 的梯度。</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@torch._disable_dynamo</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> zero_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, set_to_none: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">bool</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) -&gt; </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@overload</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> step</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, closure: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> ...</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) -&gt; </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@overload</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> step</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, closure: Callable[[], </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">float</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]) -&gt; </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">float</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 执行一次优化步骤（参数更新）。</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> step</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, closure: Optional[Callable[[], </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">float</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) -&gt; Optional[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">float</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># class:Optimizer 的 param_groups 添加一个参数组</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@torch._disable_dynamo</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> add_param_group</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, param_group: Dict[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">str</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Any]) -&gt; </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span></code></pre></div><h1 id="_3-不同实现与性能优化" tabindex="-1">3 不同实现与性能优化 <a class="header-anchor" href="#_3-不同实现与性能优化" aria-label="Permalink to &quot;3 不同实现与性能优化&quot;">​</a></h1><p>        我们的许多算法都有各种不同的实现方式，针对性能、可读性和/或通用性进行了优化，因此如果用户没有指定特定的实现方式，我们会尝试默认选择当前设备上通常最快的实现方式。<br></p><p>        我们有三个主要的实现类别：for 循环、foreach（多张量）和 fused。最直接的实现方式是对参数进行 for 循环，并进行大块的计算。相比于 for 循环，foreach 实现通常更快，它将参数组合成一个多张量，并一次性执行大块的计算，从而节省了许多连续的内核调用。我们的一些优化器甚至具有更快的fused实现方式，将大块的计算融合成一个内核。我们可以将 foreach 实现看作是在水平方向上进行融合，fused 实现则在此基础上在垂直方向上进行融合。<br></p><p>        通常情况下，这三种实现方式的性能排序为：fused -&gt; foreach -&gt; for 循环。因此，在适用的情况下，我们会默认选择 foreach 而不是 for 循环。适用的条件是 foreach 实现可用，并且用户没有指定任何与实现相关的 kwargs（例如，fused、foreach、differentiable），且所有张量都是本地的并在 CUDA 上。请注意，虽然 fused 实现比 foreach 还要快，但这些实现是较新的，我们希望在完全切换之前给它们更多的时间来适应。不过，您可以随时尝试它们！<br></p>`,39)]))}const g=i(p,[["render",l]]);export{o as __pageData,g as default};
