import{_ as e,c as o,o as t,a2 as l}from"./chunks/framework.DA-Pb-tg.js";const u=JSON.parse('{"title":"基础内容面试点","description":"","frontmatter":{},"headers":[],"relativePath":"Job_Interview/Algorithm_post/model_framework/基础内容面试点.md","filePath":"Job_Interview/Algorithm_post/model_framework/基础内容面试点.md","lastUpdated":1746438847000}'),i={name:"Job_Interview/Algorithm_post/model_framework/基础内容面试点.md"};function r(p,a,s,n,m,c){return t(),o("div",null,a[0]||(a[0]=[l('<h1 id="基础内容面试点" tabindex="-1">基础内容面试点 <a class="header-anchor" href="#基础内容面试点" aria-label="Permalink to &quot;基础内容面试点&quot;">​</a></h1><h2 id="_1-activation" tabindex="-1">1. Activation <a class="header-anchor" href="#_1-activation" aria-label="Permalink to &quot;1. Activation&quot;">​</a></h2><blockquote><p>Time: 0407</p></blockquote><ul><li><p>常用的激活函数？有什么用？</p></li><li><p>优劣势比较。</p></li><li><p>LSTM中的sigmoid和tanh啥作用？</p></li><li><p>工程中的使用？哪些更常用，为啥？</p></li><li><p>大模型的激活函数？GELU？Switch GELU？</p></li><li><p>计算公式？图像？导数及图像？等等</p></li><li><p>GELU的GLU块，以及在MLP中的门控单元。</p></li></ul><h2 id="_2-opetrator" tabindex="-1">2. Opetrator <a class="header-anchor" href="#_2-opetrator" aria-label="Permalink to &quot;2. Opetrator&quot;">​</a></h2><blockquote><p>Time：0408-0409</p></blockquote><h3 id="_2-1-norm" tabindex="-1">2.1 Norm <a class="header-anchor" href="#_2-1-norm" aria-label="Permalink to &quot;2.1 Norm&quot;">​</a></h3><ul><li><p>Norm中为啥不用BN来做NLP？ Or BN和LN的区别？（代码实现）</p><blockquote><ul><li>NLP的主要数据格式是[batch_szie, seq_len, embedding_dim]</li><li>Nlp 每个seq都是基本独立的，所以不能用Batch Norm</li><li>LN对应的维度就是对embedding_dim进行的</li></ul></blockquote></li><li><p>Norm在训练和推理时有何不同？Dropout呢？</p></li><li><p>如何实现训练和推理不同的情况？（相当于加锁或者if else）</p><blockquote><p>上面这俩可以结合torch源码解读</p></blockquote></li><li><p>BN期望和方差计算策略。</p></li><li><p>BN使用时需要注意什么？</p></li><li><p>不同的Norm的参数量分析。</p><blockquote><p>可学习参数和均值方差</p></blockquote></li><li><p>不同Norm操作维度（再联系联系其他算子及不同场景的操作维度，比如：Conv，Polling，softmax等）</p></li><li><p>其他那些常用算子的底层是copy还是in-place？Or 有哪些常用算子是in-place操作和非in-place操作？</p></li><li><p>各个大模型常用的Norm？引出RMSNorm？</p></li><li><p>RMS Norm 相比于 Layer Norm 有什么特点？公式？</p></li><li><p>Pre-Norm和Post-Norm的区别（和效果，以及大模型使用的情况）？</p><blockquote><p>需要熟悉Transformer结构和常用大模型</p></blockquote></li><li><p>DeepNorm思路？论文中的的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05278em;">β</span></span></span></span>是哪里的参数？该Norm有什么优势？伪代码实现。</p></li></ul><h3 id="_2-1-细节算子" tabindex="-1">2.1 细节算子 <a class="header-anchor" href="#_2-1-细节算子" aria-label="Permalink to &quot;2.1 细节算子&quot;">​</a></h3><blockquote><p>PyTorch细节内容</p></blockquote><ul><li><p>reshape、view、permute、transpose作用和区别。（底层细节）</p></li><li><p>torch（或者Tensor）的<code>contiguous()</code>作用？（引入上面的算子）。推理阶段为什么要确保张量连续？</p></li><li><p>Dropout训练和推理有啥不同？（联系Norm）</p></li></ul>',11)]))}const d=e(i,[["render",r]]);export{u as __pageData,d as default};
